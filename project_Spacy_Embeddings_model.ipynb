{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2.3 Spacy Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRNtTX6x9e3T"
      },
      "source": [
        "'''\n",
        "Train sense embeddings from a disambiguated corpus using Word2vec.\n",
        "'''\n",
        "import spacy\n",
        "# Load the spacy model that you have installed.\n",
        "model = spacy.load(’en_core_web_md’)\n",
        "# Process a sentence given the pre-trained model.\n",
        "embeddings = model()\n",
        "# Extract a word-vector for the 7-th word homework.\n",
        "embeddings [6]. vector\n",
        "# Get a sentence-vector as a mean of the individual word vectors.\n",
        "embeddings.vector\n",
        "\n",
        "\n",
        "#Define the definition getter function. It inputs an adjective in string form, and outputs a list of \n",
        "#parsed and pos tagged bows\n",
        "\n",
        "#Imports\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Define stopwords\n",
        "sw=stopwords.words(\"english\")\n",
        "\n",
        "#Define the lemmatizer\n",
        "wnl=nltk.WordNetLemmatizer()\n",
        "\n",
        "def definition_getter(adj):\n",
        "    adjdefs=[]                            #list to contain the adjective's definition bows\n",
        "    syns=wn.synsets(adj,pos=\"a\").copy()   #Define the wn synsets\n",
        "    \n",
        "    \n",
        "    if len(syns)==0:\n",
        "        return None\n",
        "    else:\n",
        "        for syn in syns:     #loop over the definitions (synsets)\n",
        "            adef=syn.definition()     #define the adjective definition in string form\n",
        "            pos_tagged=nltk.pos_tag(nltk.word_tokenize(adef))     #Tokenize and pos tag the sentence\n",
        "            \n",
        "            #Filter out stop words and tokens that are not alphabetical. Then filter out duplicates,\n",
        "            #then append to adjdefs\n",
        "            adjdefs.append(list(set([tup for tup in pos_tagged if tup[0] not in sw and tup[0].isalpha()])))\n",
        "    \n",
        "    #Include examples\n",
        "    exampleList=[]\n",
        "    for syn in syns:\n",
        "        ex1=\"\"\n",
        "        examples=syn.examples().copy()\n",
        "        #For each example, concatenate the strings into one large string, seperated by spaces\n",
        "        for example in examples:\n",
        "            ex1+=\" \"+example\n",
        "        pos_tagged=nltk.pos_tag(nltk.word_tokenize(ex1)) #Tokenize and pos tag the examples\n",
        "        \n",
        "        #Filter out stop words and tokens that are not alphabetical. Then filter out duplicates,\n",
        "        #then append to exampleList\n",
        "        exampleList.append(list(set([tup for tup in pos_tagged if tup[0] not in sw and tup[0].isalpha()])))\n",
        "        \n",
        "    #Concatenate each bow in adjdefs with the example bows\n",
        "    for i in range(len(adjdefs)):\n",
        "        adjdefs[i]+=exampleList[i]\n",
        "    \n",
        "    #Lemmatize the words\n",
        "    adjdefslem=[]          #list to contain lemmatized bows\n",
        "    for bow in adjdefs:    #loop over each definition in adjdefs\n",
        "        def1=[]            #list to contain lemmatized bow for a particular definition\n",
        "        for tup in bow:     #loop over each tuple in the definition (bow)\n",
        "            w=tup[0]  \n",
        "            \n",
        "            #Convert pos tags to suitable form for the lemmatizer\n",
        "            pos1=tup[1]\n",
        "            pos2=pos1[0].lower()\n",
        "            if pos2==\"j\":\n",
        "                pos2=\"a\"\n",
        "            \n",
        "            #Lemmatize the word, w\n",
        "            try:\n",
        "                lem=wnl.lemmatize(w.lower(),pos=pos2)\n",
        "            except:\n",
        "                lem=wnl.lemmatize(w.lower())\n",
        "            def1.append((lem,pos1))\n",
        "        adjdefslem.append(def1)\n",
        "    \n",
        "    #Convert pos to the lower case of the first component. This converts pos to just the main pos\n",
        "    adjdefs2=[[(tup[0],tup[1][0].lower()) for tup in definition] for definition in adjdefslem]\n",
        "    \n",
        "    #Discard duplicate words per definition\n",
        "    adjdefs3=[list(set(x)) for x in adjdefs2]\n",
        "    \n",
        "    #discard duplicate definitions\n",
        "    newlist2=[]\n",
        "    for d in adjdefs3:\n",
        "        d2=d.copy()\n",
        "        d2.sort()\n",
        "        d3=tuple(d2)\n",
        "        if d3 not in newlist2:\n",
        "            newlist2.append(d3)\n",
        "            \n",
        "    return [list(x) for x in newlist2]\n",
        "import gensim\n",
        "import sys\n",
        "import logging \n",
        "from gensim.models.word2vec import LineSentence\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "class IterableChain(object):\n",
        "    def __init__(self, iterables):\n",
        "        self.iterables = iterables\n",
        "    def __iter__(self):\n",
        "        return itertools.chain(*self.iterables)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pretrained_model_path = sys.argv[1]\n",
        "    in_path = sys.argv[2]\n",
        "    if os.path.isfile(in_path):\n",
        "        sentences = LineSentence(in_path)\n",
        "    else:\n",
        "        iters = []\n",
        "        for root, dirs, fnames in os.walk(in_path):\n",
        "            for fname in fnames:\n",
        "                if re.search(r'\\.txt\\.gz$', fname):\n",
        "                    child_path = os.path.join(root, fname)\n",
        "                    iters.append(LineSentence(child_path))\n",
        "        sentences = IterableChain(iters)\n",
        "    out_path = sys.argv[3]\n",
        "\n",
        "    # so that gensim will print something nice to the standard output\n",
        "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "    model = Word2Vec.load(pretrained_model_path)\n",
        "#     model.workers = 1 # for debugging\n",
        "    model.min_count = 1\n",
        "    model.build_vocab(sentences, sense_delimiter='---', update=True)\n",
        "    model.train(sentences, sense_delimiter='---', \n",
        "                total_examples=model.corpus_count, epochs=10)\n",
        "    model.save(out_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX21RljxAaV8"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tybvQcLFAeSZ"
      },
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Pipeline:\", nlp.pipe_names)\n",
        "doc = nlp(\"I was reading the paper.\")\n",
        "token = doc[0]  # 'I'\n",
        "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
        "print(token.morph.get(\"PronType\"))  # ['Prs']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCN8AsgQ-tSH"
      },
      "source": [
        "#Define our nouns and adjectives\n",
        "n1=\"coffee\"\n",
        "n2=\"woman\"\n",
        "adj=\"hot\"\n",
        "\n",
        "#Create nlp object by loading the lg english file (we could also load the small or medium file)\n",
        "nlp=spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "#Create nlp objects for each noun\n",
        "nlp1=nlp(n1)\n",
        "nlp2=nlp(n2)\n",
        "\n",
        "#Let d1 represent the DAV for n1 and adj, and d2 represent the DAV for n2 and adj\n",
        "\n",
        "#Loop over all definitions of adj and compute similarity scores between the noun and the definition bow.\n",
        "#Append the similarity scores to the DAVs\n",
        "d1=[]\n",
        "d2=[]\n",
        "for syn in wn.synsets(adj,pos=\"a\"):\n",
        "    nlpdef=nlp(syn.definition()) #nlp object for the definition\n",
        "    d1.append(nlp1.similarity(nlpdef)) #append similarity score to d1\n",
        "    d2.append(nlp2.similarity(nlpdef)) #append similarity score to d2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}