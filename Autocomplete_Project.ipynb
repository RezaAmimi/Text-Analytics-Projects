{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2.2/3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvyKv9XD2S1B"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "# Importing all required packages \n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0cJSmQz32Au"
      },
      "source": [
        "validate_dt=open(\"data/validate.data\",\"r\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UBYiKVH35DV"
      },
      "source": [
        "dict_parse=BeautifulSoup(dict_dt,'xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjUjcGnU7Tgu"
      },
      "source": [
        "# create a method returns the first synset of a given word\n",
        "def naive_disambiguation(word: str):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    sense = synsets[0]\n",
        "    return sense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl8dVl6A7jYE"
      },
      "source": [
        "word = 'java' # you can replae this here\n",
        "sense = naive_disambiguation(word)\n",
        "print('Definition:', sense.definition())\n",
        "print('Examples:')\n",
        "pprint.pprint(sense.examples())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYbcqMx67oEP"
      },
      "source": [
        "for synset in wordnet.synsets('java'):\n",
        "    print(synset.definition())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6wmUPPR39Ul"
      },
      "source": [
        "dictt_gloss={}\n",
        "for i in dict_parse.find_all(\"lexelt\"): \n",
        "    x=(i.get('item'))\n",
        "    for j in i.find_all(\"sense\"):        \n",
        "        z = j.get('id')\n",
        "        y=(j.get('gloss'))\n",
        "        print(x,y)\n",
        "        dictt_gloss.setdefault(x,[]).append(y)\n",
        "\n",
        "print(dictt_gloss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgRWSg6O4Btf"
      },
      "source": [
        "infile1 = open(\"data/dictionary.xml\",\"r\")\n",
        "train1 = open(\"data/train.data\",\"r\")\n",
        "test1 = open(\"data//test.data\",\"r\")\n",
        "validate1 = open(\"data/validate.data\",\"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqWhYNC_4Hga"
      },
      "source": [
        "def clean_up(inp):\n",
        "    \n",
        "    ad_s=('i','my','me','mine','said','said','also','say','u.s.','nt','much','one','m.r','m.r.','mr.','mrs.','mrs','have') # creating custom stop words to be removed\n",
        "    stopwords=nltk.corpus.stopwords.words('english')\n",
        "    stopwords.append(ad_s)\n",
        "    \n",
        "    clean_dt=[]\n",
        "    for f in str(inp).split(\" \"):\n",
        "        w=re.sub(r'[^a-zA-Z\\.\\%%]','', f)\n",
        "        w=w.lower()\n",
        "        if w not in stopwords:\n",
        "            if w != '':\n",
        "                w = porter.stem(w)\n",
        "                clean_dt.append(w)\n",
        "    return clean_dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mLuJvt4Qi3"
      },
      "source": [
        "# Reading list with all the training data\n",
        "nw_train1=train1\n",
        "train_List1 = [line.split('\\n') for line in nw_train1]\n",
        "print( train_List1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_aD4_Vq4VpR"
      },
      "source": [
        "soup = BeautifulSoup(infile,'xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkEFF7UM4Vwj"
      },
      "source": [
        "dict_gloss=dict()\n",
        "for i in soup.find_all(\"lexelt\"): \n",
        "    x=(i.get('item'))\n",
        "    for j in i.find_all(\"sense\"):  \n",
        "        z = j.get('id')\n",
        "        y=(j.get('gloss'))\n",
        "        dict_gloss.setdefault(x,[]).append(y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP5TyLyN4gGN"
      },
      "source": [
        "# get data list of words in dataframe\n",
        "dfp = pd.DataFrame(columns=['wrd','only_wrd','sense'])\n",
        "cnt=0\n",
        "for each_entry in train_List1:\n",
        "    w,ws=get_trainWord(each_entry) \n",
        "    given=w+\".\"+str(ws)\n",
        "    \n",
        "    dfp.loc[cnt]=given,w,ws\n",
        "    cnt+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDvC_bJV4kJ-"
      },
      "source": [
        "temp=(dfp[['wrd','only_wrd']].groupby(['wrd','only_wrd'])['only_wrd'].count().reset_index(name='wordSense_count'))\n",
        "temp_sens=pd.DataFrame(temp)\n",
        "temp_sens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfCkYLJk4nkX"
      },
      "source": [
        "temp2=(dfp[['only_wrd']].groupby(['only_wrd'])['only_wrd'].count().reset_index(name='wrd_count'))\n",
        "temp_wrd=pd.DataFrame(temp2)\n",
        "temp_wrd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdHBIili4sIc"
      },
      "source": [
        "for i in range(0,len(temp_sens)):\n",
        "    wrd_s=temp_sens.iloc[i][0]\n",
        "    #print(\"Sense: \",wrd_s)\n",
        "    wrd=temp_sens.iloc[i][1]\n",
        "    #print(\"Wrd: \",wrd)\n",
        "    numer=temp_sens.iloc[i][2]\n",
        "    #print(\"senseCount: \",numer)\n",
        "    deno=temp_wrd.loc[temp_wrd['only_wrd']==wrd]['wrd_count'].values[0]\n",
        "    #print(\"wrdCount: \",deno)\n",
        "    pr=numer/deno\n",
        "    #print(pr)\n",
        "    temp_prob.loc[i]=wrd_s,pr\n",
        "temp_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_dKE2e042pw"
      },
      "source": [
        "def get_featureWord(word,sent):\n",
        "    #print(\"get_featureWord : \",word,\"\\n\",sent)\n",
        "    wrd_array=[]\n",
        "    if(word in sent):\n",
        "        wr_index=sent.index(word)\n",
        "\n",
        "        iter_lst=[wr_index-2,wr_index-3,wr_index-4,wr_index-5,wr_index-6,wr_index+2,wr_index+3,wr_index+4,wr_index+5,wr_index+6]\n",
        "        not_needed=['.','%','%%','mr.','u.s.','u.s.','one','said','would','like']\n",
        "        for k in iter_lst:\n",
        "\n",
        "            try:\n",
        "                gotdata = sent[k]\n",
        "                if (gotdata not in not_needed) and len(gotdata)>3:\n",
        "                    wrd_array.append(gotdata)\n",
        "            except IndexError:\n",
        "                #gotdata = 'null'\n",
        "                pass\n",
        "    return wrd_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03wuMx6B48xP"
      },
      "source": [
        "#test\n",
        "get_featureWord('allow',al[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ME_byqn5B0C"
      },
      "source": [
        "import re\n",
        "#funtion to know the word of training, to look in dictionary\n",
        "# Pass unclean string\n",
        "def get_InputWord(inpList):\n",
        "    word_pos=re.sub(r'[^a-zA-Z\\.]','',(str((inpList)).split(\"|\"))[0])\n",
        "    word_sense=re.sub(r'[^\\d]','',(str((inpList)).split(\"|\"))[1])\n",
        "    word_context=re.sub(r'[^a-zA-Z\\.]','',(str((cl)).split(\".\"))[0])\n",
        "\n",
        "    return word_pos,word_sense,word_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MyxylW75B6X"
      },
      "source": [
        "wpos,ws,wc=get_InputWord(fst)\n",
        "print(wpos,ws,wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUI-IgnJ5Kat"
      },
      "source": [
        "# create dcitionary of training words\n",
        "dict_train=dict()\n",
        "for i in train_List1: \n",
        "    cln=clean_up(i)\n",
        "    wpos,ws,wc=get_InputnWord(i)\n",
        "    ky=wpos+\".\"+ws\n",
        "    dict_train.setdefault(ky,[]).append(cln)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8iItmUW5OQh"
      },
      "source": [
        "len(dict_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkr5idSF5KiP"
      },
      "source": [
        "# get fearue list of a word for all sentences in the data\n",
        "def allFeatureList(ky):\n",
        "    #print(\"allFeatureList : \",ky)\n",
        "    ftr_lst=[]\n",
        "    word=ky.split(\".\")[0]\n",
        "    for j in range(0,len(dict_train[ky])):\n",
        "        rel_dict=dict_train[ky][j]\n",
        "        features = get_featureWord(word,rel_dict)\n",
        "        for feature in features:\n",
        "            if feature not in ftr_lst:\n",
        "                ftr_lst.append(feature)\n",
        "                \n",
        "    return ftr_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqQ_NU_5mdn"
      },
      "source": [
        "allFeatureList('allow.v.1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO-29bDJ5pKw"
      },
      "source": [
        "def getFeatureCount(ky):\n",
        "    hm=0\n",
        "   # print(\"getFeatureCount :\" , ky)\n",
        "    featuresHere=allFeatureList(ky)\n",
        "    wrd=ky.split(\".\")[0]\n",
        "    for w in featuresHere:\n",
        "        vv=0\n",
        "        for m in (dict_train[ky]):\n",
        "            c=m.count(w)\n",
        "            vv=vv+c\n",
        "        df_test.loc[hm]=wrd,ky ,w, vv\n",
        "        hm+=1 \n",
        "\n",
        "    return df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3_-FBDH5tFp"
      },
      "source": [
        "df_test.iloc[0:0]\n",
        "dd=getFeatureCount('affect.v.1')\n",
        "#ddd=dd.set_index(['wrd','feature']).groupby('feature').nlargest(5).reset_index() \n",
        "((dd.sort_values('feature_count',ascending = False)).iloc[1:6]).reset_index(drop=True)\n",
        "#ddd.iloc[1:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px-pHFXo5yq3"
      },
      "source": [
        "########### Execution step\n",
        "# using the above function to get the count of feature word\n",
        "###########\n",
        "dfFeatureCount.iloc[0:0] # drop all rows from the data frame\n",
        "df_test.iloc[0:0] # drop all rows from the data frame\n",
        "\n",
        "df_test=pd.DataFrame(columns=['only_wrd','wrd_sense','feature','feature_count'])\n",
        "dfFeatureCount=pd.DataFrame(columns=['only_wrd','wrd_sense','feature','feature_count'])\n",
        "\n",
        "key_lst=list(dict_train.keys())\n",
        "\n",
        "for l in range(0,len(key_lst)):\n",
        "    ky=(key_lst[l])\n",
        "    #print(ky)\n",
        "    dd=getFeatureCount(ky)\n",
        "    dd=((dd.sort_values('feature_count',ascending = False)).iloc[1:7])\n",
        "    dfFeatureCount=dfFeatureCount.append(dd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIJID0mh59Uv"
      },
      "source": [
        "dfFeatureCount\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w58vXjg6Fqq"
      },
      "source": [
        "newD=dfFeatureCount.reset_index(drop=True)\n",
        "\n",
        "comData=pd.DataFrame(columns=['wrd','wrd_sns','feature','feature_count','wrdSense_count','P(f|s)'])\n",
        "\n",
        "for i in range(0,len(newD)):\n",
        "    wrd_sense=newD['wrd_sense'][i] \n",
        "    only_wrd=newD['only_wrd'][i]\n",
        "    ftr=newD['feature'][i]    \n",
        "    ftr_count=newD['feature_count'][i]\n",
        "    wrdSense_count=temp_sens[temp_sens['wrd']==wrd_sense]['wrdSense_count'].values[0]\n",
        "    pr=ftr_count/wrdSense_count\n",
        "    comData.loc[i]=only_wrd,wrd_sense,ftr,ftr_count,wrdSense_count,pr\n",
        "#print (comData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5R7W-ON6MJ5"
      },
      "source": [
        "comData #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S6Vc-Hu6PMN"
      },
      "source": [
        "from collections import Counter\n",
        "dict_train['affect.v.1'][1]\n",
        "dtSupervisedPredi=pd.DataFrame(columns=['Given','Predicted'])\n",
        "\n",
        "key_lst=list(dict_train.keys())\n",
        "sense=\"\"\n",
        "cnt=0\n",
        "\n",
        "for l in range(0,len(key_lst)):\n",
        "    \n",
        "    ky=(key_lst[l])\n",
        "\n",
        "    dtKey=dict_train[ky]\n",
        "    wrd=ky.split(\".\")[0]\n",
        "    \n",
        "    #print('key-',ky)\n",
        "    avlbl_ftr=list(comData[(comData['wrd']==wrd)]['feature'])\n",
        "    \n",
        "    for each in range(0,len(dtKey)):\n",
        "        #print(each)\n",
        "        \n",
        "        toPredict_ftr=get_featureWord(wrd,dict_train[ky][each])\n",
        "        #print(\"PredFtr\",toPredict_ftr)\n",
        "        #print(\"avlblFtr\",avlbl_ftr)\n",
        "        \n",
        "        del pr_sens[:]\n",
        "        pr_sens=[]\n",
        "        net_pr=0\n",
        "        \n",
        "        #print('word-',wrd)\n",
        "\n",
        "        for pr in toPredict_ftr:\n",
        "            prob=0\n",
        "            \n",
        "            if pr in avlbl_ftr:               \n",
        "                #select the max probability\n",
        "                prob = max(comData[(comData['wrd']==wrd)& (comData['feature']==pr)]['P(f|s)'])\n",
        "\n",
        "                #find sense for that probability and word\n",
        "                sense = comData[(comData['wrd']==wrd)& (comData['feature']==pr)]['wrd_sns'].values[0]\n",
        "                                \n",
        "            if not sense:\n",
        "                continue\n",
        "            else:\n",
        "                pr_sens.append(sense)\n",
        "\n",
        "        if not pr_sens:\n",
        "            pr_sens.append('<NotFound>')\n",
        "            \n",
        "        prediction,num_most_common=Counter(pr_sens).most_common(1)[0]\n",
        "        dtSupervisedPredi.loc[cnt]=ky,prediction\n",
        "        cnt +=1\n",
        "        #print(\"Given: \",ky,\"Predicted: \",prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEdoasds6QAN"
      },
      "source": [
        "# Supervised Model\n",
        "dtSupervisedPredi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShtHe0F_6W_8"
      },
      "source": [
        "accuracies = {}\n",
        "\n",
        "def accuracy(df):\n",
        "    res = (df['Given'] == df['Predicted']).values\n",
        "    return ((res==True).sum()/float(len(res))) * 100  \n",
        "\n",
        "accuracies['SupervisedModel'] = accuracy(dtSupervisedPredi)\n",
        "\n",
        "print(\"{\" + \"\\n\".join(\"{}: {}\".format(k, v) for k, v in accuracies.items()) + \"}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}