{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "porter=PorterStemmer()\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = open(\"dictionary.xml\",\"r\")\n",
    "soup = BeautifulSoup(doc,'lxml')\n",
    "\n",
    "dict_gloss = dict()\n",
    "for i in soup.find_all(\"lexelt\"): \n",
    "    x=(i.get('item'))\n",
    "    for j in i.find_all(\"sense\"):  \n",
    "        y=(j.get('gloss'))\n",
    "        dict_gloss.setdefault(x,[]).append(y) \n",
    "        \n",
    "dict_eg=dict()\n",
    "for i in soup.find_all(\"lexelt\"): \n",
    "    x=(i.get('item'))\n",
    "    for j in i.find_all(\"sense\"):  \n",
    "        y=(j.get('examples'))\n",
    "        dict_eg.setdefault(x,[]).append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainWord(inpList):\n",
    "    word=re.sub(r'[^a-zA-Z\\.]','',(str((inpList)).split(\"|\"))[0])\n",
    "    word_sense=re.sub(r'[^\\d]','',(str((inpList)).split(\"|\"))[1])\n",
    "    return word,word_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_s=('i','I','my','me','mine') # creating custom stop words to be removed\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stopwords.append(ad_s)\n",
    "\n",
    "def clean_up(inp):\n",
    "    clean_dt=[]\n",
    "    for f in str(inp).split(\" \"):\n",
    "        w=re.sub(r'[^a-zA-Z\\.\\%%]','', f)\n",
    "        w=w.lower()\n",
    "        if w not in stopwords:\n",
    "            if w != '':\n",
    "                w = porter.stem(w)\n",
    "                clean_dt.append(w)\n",
    "    return clean_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams(lw):\n",
    "    bi_words = []\n",
    "    for i in range(0,len(lw)-1):\n",
    "        bi_words.append(lw[i]+' '+lw[i+1])\n",
    "    return bi_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"train.data\",\"r\")\n",
    "test = open(\"test.data\",\"r\")\n",
    "validate = open(\"validate.data\",\"r\")\n",
    "nw_train = train\n",
    "train_List = [line.split('\\n') for line in nw_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlapcontext(word,sense_num, sentence):\n",
    "\n",
    "    gloss = []\n",
    "    gloss = clean_up(dict_gloss[word][sense_num])\n",
    "    gloss.extend(clean_up(dict_eg[word][sense_num]))\n",
    "    gloss_bi = create_bigrams(gloss)\n",
    "    \n",
    "    sent = clean_up(sentence)\n",
    "    sent_bi = create_bigrams(sent)\n",
    "   \n",
    "    return len(set(gloss).intersection(set(sent))), len(set(gloss_bi).intersection(set(sent_bi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence, cnt):\n",
    "    bestsense = None\n",
    "    bestsense_bi = None\n",
    "    maxoverlap = 0\n",
    "    maxoverlap_bi = 0\n",
    "    \n",
    "    for sense_num in range(0,len(dict_gloss[word])):\n",
    "        overlap, overlap_bi = overlapcontext(word,sense_num,sentence)\n",
    "        \n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = word+'.'+str(sense_num+1)\n",
    "            \n",
    "        if (overlap_bi!=0) and (overlap_bi > maxoverlap_bi):\n",
    "            bestsense_bi = word+'.'+str(sense_num+1)\n",
    "            \n",
    "    if overlap_bi>0:\n",
    "        bestsense = bestsense_bi\n",
    "        \n",
    "            \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict=pd.DataFrame(columns=['Given','Predicted'])\n",
    "df_predict_Eg=pd.DataFrame(columns=('Given','Predicted'))\n",
    "df_predict_Gloss=pd.DataFrame(columns=('Given','Predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt= 0\n",
    "for each_entry in train_List:\n",
    "    w,ws=get_trainWord(each_entry) \n",
    "    clean_entry=(clean_up(each_entry))\n",
    "    predicted = lesk(w,clean_entry,cnt)\n",
    "    given=w+\".\"+str(ws)\n",
    "    df_predict.loc[cnt]=given,predicted\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    res = (df['Given'] == df['Predicted']).values\n",
    "    return ((res==True).sum()/float(len(res))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.77783761949644"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2ddfe57159f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0meach_entry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_List\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trainWord_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclean_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_entry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_List' is not defined"
     ]
    }
   ],
   "source": [
    "dict_eg_new = dict()\n",
    "\n",
    "def get_trainWord_new(inpList):\n",
    "    word=re.sub(r'[^a-zA-Z\\.]','',(str((inpList)).split(\"|\"))[0])\n",
    "    return word\n",
    "\n",
    "for each_entry in train_List:\n",
    "    w = get_trainWord_new(each_entry)\n",
    "    clean_entry = (clean_up(each_entry))\n",
    "    dict_eg_new[w] = clean_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlapcontext1(word,sense_num, sentence):\n",
    "\n",
    "    gloss = []\n",
    "    gloss = clean_up(dict_gloss[word][sense_num])\n",
    "    gloss.extend(clean_up(dict_eg[word][sense_num]))\n",
    "    gloss.extend(dict_eg_new[word])\n",
    "    gloss_bi = create_bigrams(gloss)\n",
    "    \n",
    "    sent = clean_up(sentence)\n",
    "    sent_bi = create_bigrams(sent)\n",
    "   \n",
    "    return len(set(gloss).intersection(set(sent))), len(set(gloss_bi).intersection(set(sent_bi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpuslesk(word, sentence, cnt):\n",
    "    bestsense = None\n",
    "    bestsense_bi = None\n",
    "    maxoverlap = 0\n",
    "    maxoverlap_bi = 0\n",
    "    \n",
    "    for sense_num in range(0,len(dict_gloss[word])):\n",
    "        overlap, overlap_bi = overlapcontext1(word,sense_num,sentence)\n",
    "        \n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = word+'.'+str(sense_num+1)\n",
    "            \n",
    "        if (overlap_bi!=0) and (overlap_bi > maxoverlap_bi):\n",
    "            bestsense_bi = word+'.'+str(sense_num+1)\n",
    "            \n",
    "    if overlap_bi>0:\n",
    "        bestsense = bestsense_bi\n",
    "        \n",
    "            \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_List = [line.split('\\n') for line in validate]\n",
    "test_List = [line.split('\\n') for line in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_val=pd.DataFrame(columns=['Given','Predicted'])\n",
    "df_predict_Eg_val=pd.DataFrame(columns=('Given','Predicted'))\n",
    "df_predict_Gloss_val=pd.DataFrame(columns=('Given','Predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_test=pd.DataFrame(columns=['Given','Predicted'])\n",
    "df_predict_Eg_test=pd.DataFrame(columns=('Given','Predicted'))\n",
    "df_predict_Gloss_test=pd.DataFrame(columns=('Given','Predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt= 0\n",
    "for each_entry in val_List:\n",
    "    w,ws=get_trainWord(each_entry) \n",
    "    clean_entry=(clean_up(each_entry))\n",
    "    predicted = corpuslesk(w,clean_entry,cnt)\n",
    "    given=w+\".\"+str(ws)\n",
    "    df_predict_val.loc[cnt]=given,predicted\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.326902465166132"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df_predict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt= 0\n",
    "for each_entry in test_List:\n",
    "    w,ws=get_trainWord(each_entry) \n",
    "    clean_entry=(clean_up(each_entry))\n",
    "    predicted = corpuslesk(w,clean_entry,cnt)\n",
    "    given=w+\".\"+str(ws)\n",
    "    df_predict_test.loc[cnt]=given,predicted\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(df_predict_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
